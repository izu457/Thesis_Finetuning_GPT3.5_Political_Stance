{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code from OpenAI Cookbook\n",
    "https://cookbook.openai.com/examples/chat_finetuning_data_prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import tiktoken # for token counting\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validate Data Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples: 10\n",
      "First example:\n",
      "{'role': 'system', 'content': 'Imagine you are a member of the European\\n         Parliament and based on your years of experience, you are an expert in \\n         predicting how the different party groups will vote on a given law. Given\\n         the above legislative proposal, predict the percentage of votes in favour \\n         from each party group in the European Parliament. Assess the political \\n         direction, wording, framing, and topic relevance of the law to inform your \\n         predictions. Determine the type of majority (General, Left, Right, Consensus) \\n         likely to support the legislation based on party alignments.'}\n",
      "{'role': 'user', 'content': 'Amendment of the MFF 2014-2020: PURPOSE: to amend Regulation (EU, Euratom) No 1311/2013 laying down the multiannual financial framework for the years 2014-2020. PROPOSED ACT: Council Regulation. ROLE OF THE EUROPEAN PARLIAMENT: Council may adopt the act only if Parliament has given its consent to the act. CONTENT: Article 19 of Council Regulation No 1311/2013 stipulates that in the event of the adoption after 1 January 2014 of new rules or programmes under shared management for the Structural Funds, the Cohesion Fund, the European Agricultural Fund for Rural Development, the European Maritime and Fisheries Fund, the Asylum and Migration Fund and the Internal Security Fund, the MFF shall be revised in order to transfer to subsequent years, in excess of the corresponding expenditure ceilings, allocations not used in 2014. The revision concerning the transfer of unused allocation for the year 2014 shall be adopted before 1 May 2015. It thus applies to programmes adopted after 1 January 2014 even when the relevant legal act was adopted by that time. Moreover, ‘rules’ covers not only the basic legislative acts laying down the provisions for implementing the funds concerned, but also implementing and delegated acts, to the extent to which they are a prerequisite for preparing or finalising the programmes. Consequently, this provision also applies to funds from the specific allocation for the Youth Employment Initiative as the legal basis is the same as for programmes. It also applies to the Fund for European Aid to the most Deprived (FEAD) as its commitments originate from the Structural Funds and are implemented under shared management. Article 19 also covers contributions from the ERDF to the cross-border and sea-basin programmes established under the European Neighbourhood Instrument and the Instrument for Pre-Accession Assistance as those amounts are part of national allocations defined in Article 91(2) of the ESI Funds’ Common Provisions Regulation (CPR). It should be noted that Article 19 does not impose any constraints as to the profile of the transfer of allocations to subsequent years. Amounts concerned: as a result of such late adoption: This proposal seeks to amend Annex to Regulation (EU, Euratom) No 1311/2013 by transferring the commitment appropriations not used in 2014 to subsequent years for sub-heading 1b, heading 2 and heading 3. N.B. the Commission proposes to transfer the bulk of the allocations not used in 2014 to year 2015 in order to keep the pace of investments for growth and jobs, minimise differences of treatment with programmes adopted in 2014 and ensure equal treatment with programmes whose 2014 commitment tranche is carried-over in accordance with Article 13 of the Financial Regulation. An annex sets outs details of the amounts transferred by the Funds concerned and by year of transfer. IMPLICATIONS ON PAYMENTS: concerning the implications on payments in 2015, they will be covered within the voted budget for 2015. The first initial pre-financing that has not been paid in 2014 will have to be paid in 2015 together with the second pre-financing. However, the corresponding unused appropriations in 2014 have been used by transfers to reduce the backlog of unpaid bills from the previous period 2007-2013 and the reverse operation could be done, if need be, in 2015 to cover the pre-financings. The medium and longer term implications on interim payments of the transfer are more difficult to predict. The legislative acts laying down the provisions for implementing the funds contain provisions for the automatic decommitment of appropriations not used within a given deadline, which is of n+3 years for the ESI Funds and of n+2 years for the AMIF and the ISF. This could in principle result in a shift of payments from one year to another without this lowering the overall needs over the 2014-2020 period. On the other hand, the real pace of implementation will not be pre-determined by the transfer. For these reasons, the Commission does not propose to revise the payment ceilings. It will review the situation regularly in the light of implementation and make proposals if appropriate in accordance with the relevant provisions of the MFF Regulation.'}\n",
      "{'role': 'assistant', 'content': 'ECR%:0.871, EPP%:0.864, EFD/IDG%:0.375, Greens/EFA%:0.92, NI%:0.212, REG%:0.806, S&D%:0.901, The Left%:0.731, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0'}\n"
     ]
    }
   ],
   "source": [
    "# load jsonl file\n",
    "data_path = \"data/finetuning_data/small_training_data.jsonl\"\n",
    "\n",
    "# Load the dataset\n",
    "with open(data_path, 'r', encoding='utf-8') as f:\n",
    "    dataset = [json.loads(line) for line in f]\n",
    "\n",
    "# Initial dataset stats\n",
    "print(\"Num examples:\", len(dataset))\n",
    "print(\"First example:\")\n",
    "for message in dataset[0][\"messages\"]:\n",
    "    print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No errors found\n"
     ]
    }
   ],
   "source": [
    "# Format error checks\n",
    "format_errors = defaultdict(int)\n",
    "\n",
    "for ex in dataset:\n",
    "    if not isinstance(ex, dict):\n",
    "        format_errors[\"data_type\"] += 1\n",
    "        continue\n",
    "        \n",
    "    messages = ex.get(\"messages\", None)\n",
    "    if not messages:\n",
    "        format_errors[\"missing_messages_list\"] += 1\n",
    "        continue\n",
    "        \n",
    "    for message in messages:\n",
    "        if \"role\" not in message or \"content\" not in message:\n",
    "            format_errors[\"message_missing_key\"] += 1\n",
    "        \n",
    "        if any(k not in (\"role\", \"content\", \"name\", \"function_call\", \"weight\") for k in message):\n",
    "            format_errors[\"message_unrecognized_key\"] += 1\n",
    "        \n",
    "        if message.get(\"role\", None) not in (\"system\", \"user\", \"assistant\", \"function\"):\n",
    "            format_errors[\"unrecognized_role\"] += 1\n",
    "            \n",
    "        content = message.get(\"content\", None)\n",
    "        function_call = message.get(\"function_call\", None)\n",
    "        \n",
    "        if (not content and not function_call) or not isinstance(content, str):\n",
    "            format_errors[\"missing_content\"] += 1\n",
    "    \n",
    "    if not any(message.get(\"role\", None) == \"assistant\" for message in messages):\n",
    "        format_errors[\"example_missing_assistant_message\"] += 1\n",
    "\n",
    "if format_errors:\n",
    "    print(\"Found errors:\")\n",
    "    for k, v in format_errors.items():\n",
    "        print(f\"{k}: {v}\")\n",
    "else:\n",
    "    print(\"No errors found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS\n",
    "\n",
    "# cl100k_base is the tokenizer used by ChatGPT3.5 and ChatGPT4\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "\n",
    "# not exact!\n",
    "# simplified from https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb\n",
    "def num_tokens_from_messages(messages, tokens_per_message=3, tokens_per_name=1):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        num_tokens += tokens_per_message\n",
    "        for key, value in message.items():\n",
    "            # .encode returns the list of tokens (so that you can count them with len())\n",
    "            num_tokens += len(encoding.encode(value))\n",
    "            if key == \"name\":\n",
    "                num_tokens += tokens_per_name\n",
    "    num_tokens += 3\n",
    "    return num_tokens\n",
    "\n",
    "def num_assistant_tokens_from_messages(messages):\n",
    "    num_tokens = 0\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            num_tokens += len(encoding.encode(message[\"content\"]))\n",
    "    return num_tokens\n",
    "\n",
    "def print_distribution(values, name):\n",
    "    print(f\"\\n#### Distribution of {name}:\")\n",
    "    print(f\"min / max: {min(values)}, {max(values)}\")\n",
    "    print(f\"mean / median: {np.mean(values)}, {np.median(values)}\")\n",
    "    print(f\"p5 / p95: {np.quantile(values, 0.1)}, {np.quantile(values, 0.9)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num examples missing system message: 0\n",
      "Num examples missing user message: 0\n",
      "\n",
      "#### Distribution of num_messages_per_example:\n",
      "min / max: 3, 3\n",
      "mean / median: 3.0, 3.0\n",
      "p5 / p95: 3.0, 3.0\n",
      "\n",
      "#### Distribution of num_assistant_tokens_per_example:\n",
      "min / max: 85, 85\n",
      "mean / median: 85.0, 85.0\n",
      "p5 / p95: 85.0, 85.0\n",
      "\n",
      "0 examples may be over the 4096 token limit, they will be truncated during fine-tuning\n"
     ]
    }
   ],
   "source": [
    "# check for missing data, distribution of messages in each conversation,\n",
    "# distribution of tokens per conversation, print token limit warnings\n",
    "\n",
    "# Warnings and tokens counts\n",
    "n_missing_system = 0\n",
    "n_missing_user = 0\n",
    "n_messages = []\n",
    "convo_lens = []\n",
    "assistant_message_lens = []\n",
    "\n",
    "for ex in dataset:\n",
    "    messages = ex[\"messages\"]\n",
    "    if not any(message[\"role\"] == \"system\" for message in messages):\n",
    "        n_missing_system += 1\n",
    "    if not any(message[\"role\"] == \"user\" for message in messages):\n",
    "        n_missing_user += 1\n",
    "    n_messages.append(len(messages))\n",
    "    convo_lens.append(num_tokens_from_messages(messages))\n",
    "    assistant_message_lens.append(num_assistant_tokens_from_messages(messages))\n",
    "    \n",
    "print(\"Num examples missing system message:\", n_missing_system)\n",
    "print(\"Num examples missing user message:\", n_missing_user)\n",
    "print_distribution(n_messages, \"num_messages_per_example\")\n",
    "#print_distribution(convo_lens, \"num_total_tokens_per_example\")\n",
    "print_distribution(assistant_message_lens, \"num_assistant_tokens_per_example\")\n",
    "n_too_long = sum(l > 4096 for l in convo_lens)\n",
    "print(f\"\\n{n_too_long} examples may be over the 4096 token limit, they will be truncated during fine-tuning\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimate costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset has ~10626 tokens that will be charged for during training\n",
      "By default, you'll train for 10 epochs on this dataset\n",
      "By default, you'll be charged for ~106260 tokens\n"
     ]
    }
   ],
   "source": [
    "# Pricing and default n_epochs estimate\n",
    "MAX_TOKENS_PER_EXAMPLE = 4096\n",
    "\n",
    "TARGET_EPOCHS = 3\n",
    "MIN_TARGET_EXAMPLES = 100\n",
    "MAX_TARGET_EXAMPLES = 25000\n",
    "MIN_DEFAULT_EPOCHS = 1\n",
    "MAX_DEFAULT_EPOCHS = 25\n",
    "\n",
    "n_epochs = TARGET_EPOCHS\n",
    "n_train_examples = len(dataset)\n",
    "if n_train_examples * TARGET_EPOCHS < MIN_TARGET_EXAMPLES:\n",
    "    n_epochs = min(MAX_DEFAULT_EPOCHS, MIN_TARGET_EXAMPLES // n_train_examples)\n",
    "elif n_train_examples * TARGET_EPOCHS > MAX_TARGET_EXAMPLES:\n",
    "    n_epochs = max(MIN_DEFAULT_EPOCHS, MAX_TARGET_EXAMPLES // n_train_examples)\n",
    "\n",
    "n_billing_tokens_in_dataset = sum(min(MAX_TOKENS_PER_EXAMPLE, length) for length in convo_lens)\n",
    "print(f\"Dataset has ~{n_billing_tokens_in_dataset} tokens that will be charged for during training\")\n",
    "print(f\"By default, you'll train for {n_epochs} epochs on this dataset\")\n",
    "print(f\"By default, you'll be charged for ~{n_epochs * n_billing_tokens_in_dataset} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload validated files to OpenAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded training file id file-reEKt34LG2BCnkhO2WtDTifP\n",
      "Uploaded validation file id file-A6Znpu66UheqOTqqkSaXjuiF\n"
     ]
    }
   ],
   "source": [
    "# upload validated data file to OpenAI API\n",
    "# set OPENAI_API_KEY in environment variables\n",
    "\n",
    "client = openai.OpenAI(api_key=os.environ['TestKey3'])\n",
    "\n",
    "train_upload = client.files.create(\n",
    "  file=open(\"small_training_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "print(\"Uploaded training file id\", train_upload.id)\n",
    "\n",
    "val_upload = client.files.create(\n",
    "  file=open(\"small_validation_data.jsonl\", \"rb\"),\n",
    "  purpose=\"fine-tune\"\n",
    ")\n",
    "print(\"Uploaded validation file id\", val_upload.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To Do:\n",
    "- Was sind die verschiedenen Arten von Loss?\n",
    "- Was sind die Steps? Wie kriege ich den loss runter?\n",
    "- eine Art finden, loss usw kontrollieren - Website\n",
    "- einen Algorithmus zum Testen verschiedener Hyperparameter bauen\n",
    "- Ergbenis verschiedenerHyperparameter visualisieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Fine-Tuning Job via OpenAI Software Development Kit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tunning model with jobID: ftjob-yopylfrAJb6XAXOXWCQOMIzO.\n",
      "Training Response: FineTuningJob(id='ftjob-yopylfrAJb6XAXOXWCQOMIzO', created_at=1721253198, error=Error(code=None, message=None, param=None), fine_tuned_model=None, finished_at=None, hyperparameters=Hyperparameters(n_epochs=3, batch_size=1, learning_rate_multiplier=0.01), model='gpt-3.5-turbo-0125', object='fine_tuning.job', organization_id='org-URxBkHYInUDxHdJfjeVT2W58', result_files=[], seed=124, status='validating_files', trained_tokens=None, training_file='file-reEKt34LG2BCnkhO2WtDTifP', validation_file='file-A6Znpu66UheqOTqqkSaXjuiF', estimated_finish=None, integrations=[], user_provided_suffix='small')\n",
      "Training Status: validating_files\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "SyncCursorPage[FineTuningJobEvent](data=[FineTuningJobEvent(id='ftevent-6aMlk30rPhoJ8hexulyuBzz3', created_at=1721253198, level='info', message='Validating training file: file-reEKt34LG2BCnkhO2WtDTifP and validation file: file-A6Znpu66UheqOTqqkSaXjuiF', object='fine_tuning.job.event', data={}, type='message'), FineTuningJobEvent(id='ftevent-swxnxfTGtfSaPoOUFt8qzhau', created_at=1721253198, level='info', message='Created fine-tuning job: ftjob-yopylfrAJb6XAXOXWCQOMIzO', object='fine_tuning.job.event', data={}, type='message')], object='list', has_more=False)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_rate_multiplier = [0.1, 0.01, 0.001, 0.0001, 0.00001]\n",
    "\n",
    "response = client.fine_tuning.jobs.create(\n",
    "  training_file=train_upload.id, # file id returned after upload to API \n",
    "  validation_file=val_upload.id, # file id returned after upload to API\n",
    "  model=\"gpt-3.5-turbo\",\n",
    "  suffix=\"small\",\n",
    "  seed=124,\n",
    "  hyperparameters={\n",
    "  \"n_epochs\": 3,\n",
    "\t\"batch_size\": 1,\n",
    "\t\"learning_rate_multiplier\": 0.01\n",
    "  }\n",
    ")\n",
    "job_id = response.id\n",
    "\n",
    "print(f'Fine-tunning model with jobID: {job_id}.')\n",
    "print(f\"Training Response: {response}\")\n",
    "print(f\"Training Status: {response.status}\")\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(job_id)\n",
    "\n",
    "# monitor training progress, get ft events\n",
    "client.fine_tuning.jobs.list_events(\n",
    "  fine_tuning_job_id=job_id,\n",
    "  limit=10\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step,train_loss,train_accuracy,valid_loss,valid_mean_token_accuracy\n",
      "1,2.72811,0.65517,,\n",
      "2,2.48728,0.63218,,\n",
      "3,2.38815,0.63218,,\n",
      "4,2.39879,0.64368,,\n",
      "5,2.38417,0.64368,,\n",
      "6,1.9783,0.70115,,\n",
      "7,1.8129,0.67816,,\n",
      "8,1.69734,0.70115,,\n",
      "9,1.52965,0.73563,,\n",
      "10,1.47979,0.74713,,\n",
      "11,1.38282,0.77011,,\n",
      "12,1.27703,0.7931,,\n",
      "13,1.4016,0.77011,,\n",
      "14,1.18338,0.81609,,\n",
      "15,1.08778,0.8046,,\n",
      "16,0.96242,0.83908,,\n",
      "17,0.8915,0.85057,,\n",
      "18,0.83905,0.86207,,\n",
      "19,0.88796,0.86207,,\n",
      "20,0.73361,0.88506,,\n",
      "21,0.74653,0.89655,,\n",
      "22,0.78271,0.87356,,\n",
      "23,0.6437,0.89655,,\n",
      "24,0.60303,0.88506,,\n",
      "25,0.58872,0.88506,,\n",
      "26,0.57923,0.89655,,\n",
      "27,0.70565,0.89655,,\n",
      "28,0.54588,0.89655,,\n",
      "29,0.55097,0.89655,,\n",
      "30,0.58793,0.89655,,\n",
      "31,0.48633,0.90805,,\n",
      "32,0.4856,0.91954,,\n",
      "33,0.48564,0.90805,,\n",
      "34,0.47867,0.90805,,\n",
      "35,0.54763,0.89655,,\n",
      "36,0.69031,0.88506,,\n",
      "37,0.55866,0.89655,,\n",
      "38,0.52637,0.90805,,\n",
      "39,0.54039,0.91954,,\n",
      "40,0.61456,0.89655,,\n",
      "41,0.47817,0.89655,,\n",
      "42,0.49029,0.91954,,\n",
      "43,0.58326,0.89655,,\n",
      "44,0.50211,0.90805,,\n",
      "45,0.42614,0.90805,,\n",
      "46,0.45966,0.90805,,\n",
      "47,0.50521,0.89655,,\n",
      "48,0.52857,0.90805,,\n",
      "49,0.49486,0.91954,,\n",
      "50,0.4318,0.91954,,\n",
      "51,0.49124,0.90805,,\n",
      "52,0.41945,0.91954,,\n",
      "53,0.45719,0.90805,,\n",
      "54,0.4694,0.90805,,\n",
      "55,0.46479,0.90805,,\n",
      "56,0.56709,0.88506,,\n",
      "57,0.41312,0.90805,,\n",
      "58,0.43585,0.91954,,\n",
      "59,0.44938,0.91954,,\n",
      "60,0.4052,0.91954,,\n",
      "61,0.39214,0.91954,,\n",
      "62,0.38511,0.91954,,\n",
      "63,0.41365,0.91954,,\n",
      "64,0.41899,0.90805,,\n",
      "65,0.33718,0.90805,,\n",
      "66,0.39635,0.90805,,\n",
      "67,0.4668,0.91954,,\n",
      "68,0.39164,0.90805,,\n",
      "69,0.3707,0.91954,,\n",
      "70,0.59485,0.90805,,\n",
      "71,0.35557,0.90805,,\n",
      "72,0.343,0.91954,,\n",
      "73,0.40132,0.90805,,\n",
      "74,0.34526,0.90805,,\n",
      "75,0.30257,0.90805,,\n",
      "76,0.41873,0.93103,,\n",
      "77,0.31682,0.93103,,\n",
      "78,0.46104,0.91954,,\n",
      "79,0.28527,0.93103,,\n",
      "80,0.51798,0.89655,,\n",
      "81,0.27442,0.93103,,\n",
      "82,0.33407,0.90805,,\n",
      "83,0.48548,0.91954,,\n",
      "84,0.32911,0.94253,,\n",
      "85,0.25998,0.91954,,\n",
      "86,0.26014,0.93103,,\n",
      "87,0.26283,0.93103,,\n",
      "88,0.28438,0.91954,,\n",
      "89,0.41951,0.91954,,\n",
      "90,0.24684,0.93103,,\n",
      "91,0.22497,0.95402,,\n",
      "92,0.24364,0.94253,,\n",
      "93,0.2434,0.94253,,\n",
      "94,0.22136,0.93103,,\n",
      "95,0.27046,0.93103,,\n",
      "96,0.27065,0.93103,,\n",
      "97,0.29546,0.94253,,\n",
      "98,0.21152,0.93103,,\n",
      "99,0.36374,0.91954,,\n",
      "100,0.40316,0.90805,,\n",
      "\n",
      "b'c3RlcCx0cmFpbl9sb3NzLHRyYWluX2FjY3VyYWN5LHZhbGlkX2xvc3MsdmFsaWRfbWVhbl90b2tlbl9hY2N1cmFjeQoxLDIuNzI4MTEsMC42NTUxNywsCjIsMi40ODcyOCwwLjYzMjE4LCwKMywyLjM4ODE1LDAuNjMyMTgsLAo0LDIuMzk4NzksMC42NDM2OCwsCjUsMi4zODQxNywwLjY0MzY4LCwKNiwxLjk3ODMsMC43MDExNSwsCjcsMS44MTI5LDAuNjc4MTYsLAo4LDEuNjk3MzQsMC43MDExNSwsCjksMS41Mjk2NSwwLjczNTYzLCwKMTAsMS40Nzk3OSwwLjc0NzEzLCwKMTEsMS4zODI4MiwwLjc3MDExLCwKMTIsMS4yNzcwMywwLjc5MzEsLAoxMywxLjQwMTYsMC43NzAxMSwsCjE0LDEuMTgzMzgsMC44MTYwOSwsCjE1LDEuMDg3NzgsMC44MDQ2LCwKMTYsMC45NjI0MiwwLjgzOTA4LCwKMTcsMC44OTE1LDAuODUwNTcsLAoxOCwwLjgzOTA1LDAuODYyMDcsLAoxOSwwLjg4Nzk2LDAuODYyMDcsLAoyMCwwLjczMzYxLDAuODg1MDYsLAoyMSwwLjc0NjUzLDAuODk2NTUsLAoyMiwwLjc4MjcxLDAuODczNTYsLAoyMywwLjY0MzcsMC44OTY1NSwsCjI0LDAuNjAzMDMsMC44ODUwNiwsCjI1LDAuNTg4NzIsMC44ODUwNiwsCjI2LDAuNTc5MjMsMC44OTY1NSwsCjI3LDAuNzA1NjUsMC44OTY1NSwsCjI4LDAuNTQ1ODgsMC44OTY1NSwsCjI5LDAuNTUwOTcsMC44OTY1NSwsCjMwLDAuNTg3OTMsMC44OTY1NSwsCjMxLDAuNDg2MzMsMC45MDgwNSwsCjMyLDAuNDg1NiwwLjkxOTU0LCwKMzMsMC40ODU2NCwwLjkwODA1LCwKMzQsMC40Nzg2NywwLjkwODA1LCwKMzUsMC41NDc2MywwLjg5NjU1LCwKMzYsMC42OTAzMSwwLjg4NTA2LCwKMzcsMC41NTg2NiwwLjg5NjU1LCwKMzgsMC41MjYzNywwLjkwODA1LCwKMzksMC41NDAzOSwwLjkxOTU0LCwKNDAsMC42MTQ1NiwwLjg5NjU1LCwKNDEsMC40NzgxNywwLjg5NjU1LCwKNDIsMC40OTAyOSwwLjkxOTU0LCwKNDMsMC41ODMyNiwwLjg5NjU1LCwKNDQsMC41MDIxMSwwLjkwODA1LCwKNDUsMC40MjYxNCwwLjkwODA1LCwKNDYsMC40NTk2NiwwLjkwODA1LCwKNDcsMC41MDUyMSwwLjg5NjU1LCwKNDgsMC41Mjg1NywwLjkwODA1LCwKNDksMC40OTQ4NiwwLjkxOTU0LCwKNTAsMC40MzE4LDAuOTE5NTQsLAo1MSwwLjQ5MTI0LDAuOTA4MDUsLAo1MiwwLjQxOTQ1LDAuOTE5NTQsLAo1MywwLjQ1NzE5LDAuOTA4MDUsLAo1NCwwLjQ2OTQsMC45MDgwNSwsCjU1LDAuNDY0NzksMC45MDgwNSwsCjU2LDAuNTY3MDksMC44ODUwNiwsCjU3LDAuNDEzMTIsMC45MDgwNSwsCjU4LDAuNDM1ODUsMC45MTk1NCwsCjU5LDAuNDQ5MzgsMC45MTk1NCwsCjYwLDAuNDA1MiwwLjkxOTU0LCwKNjEsMC4zOTIxNCwwLjkxOTU0LCwKNjIsMC4zODUxMSwwLjkxOTU0LCwKNjMsMC40MTM2NSwwLjkxOTU0LCwKNjQsMC40MTg5OSwwLjkwODA1LCwKNjUsMC4zMzcxOCwwLjkwODA1LCwKNjYsMC4zOTYzNSwwLjkwODA1LCwKNjcsMC40NjY4LDAuOTE5NTQsLAo2OCwwLjM5MTY0LDAuOTA4MDUsLAo2OSwwLjM3MDcsMC45MTk1NCwsCjcwLDAuNTk0ODUsMC45MDgwNSwsCjcxLDAuMzU1NTcsMC45MDgwNSwsCjcyLDAuMzQzLDAuOTE5NTQsLAo3MywwLjQwMTMyLDAuOTA4MDUsLAo3NCwwLjM0NTI2LDAuOTA4MDUsLAo3NSwwLjMwMjU3LDAuOTA4MDUsLAo3NiwwLjQxODczLDAuOTMxMDMsLAo3NywwLjMxNjgyLDAuOTMxMDMsLAo3OCwwLjQ2MTA0LDAuOTE5NTQsLAo3OSwwLjI4NTI3LDAuOTMxMDMsLAo4MCwwLjUxNzk4LDAuODk2NTUsLAo4MSwwLjI3NDQyLDAuOTMxMDMsLAo4MiwwLjMzNDA3LDAuOTA4MDUsLAo4MywwLjQ4NTQ4LDAuOTE5NTQsLAo4NCwwLjMyOTExLDAuOTQyNTMsLAo4NSwwLjI1OTk4LDAuOTE5NTQsLAo4NiwwLjI2MDE0LDAuOTMxMDMsLAo4NywwLjI2MjgzLDAuOTMxMDMsLAo4OCwwLjI4NDM4LDAuOTE5NTQsLAo4OSwwLjQxOTUxLDAuOTE5NTQsLAo5MCwwLjI0Njg0LDAuOTMxMDMsLAo5MSwwLjIyNDk3LDAuOTU0MDIsLAo5MiwwLjI0MzY0LDAuOTQyNTMsLAo5MywwLjI0MzQsMC45NDI1MywsCjk0LDAuMjIxMzYsMC45MzEwMywsCjk1LDAuMjcwNDYsMC45MzEwMywsCjk2LDAuMjcwNjUsMC45MzEwMywsCjk3LDAuMjk1NDYsMC45NDI1MywsCjk4LDAuMjExNTIsMC45MzEwMywsCjk5LDAuMzYzNzQsMC45MTk1NCwsCjEwMCwwLjQwMzE2LDAuOTA4MDUsLAo='\n"
     ]
    }
   ],
   "source": [
    "# job_response = client.fine_tuning.jobs.retrieve(\"ftjob-379I6N5AF7iFRwplI51XI8jW\")\n",
    "\n",
    "# content = client.files.content(job_response.result_files[0])\n",
    "\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer sk-proj-GL73kbRwhRpgN3EmXz1YT3BlbkFJEMJhTsinxQDel42BZdNz'\n",
    "}\n",
    "try:\n",
    "    response = requests.get(\"https://api.openai.com/v1/files/file-XHXiH7MSUdItVNo00qj1tIE2/content\", headers=headers)\n",
    "    response.raise_for_status()  # Raises HTTPError for bad responses (4xx and 5xx)\n",
    "    if response.content:\n",
    "        try:\n",
    "            # Decode the Base64 encoded content\n",
    "            decoded_content = base64.b64decode(response.content).decode('utf-8')\n",
    "            # Display the decoded content\n",
    "            print(decoded_content)\n",
    "        except (ValueError, base64.binascii.Error):\n",
    "            # Handle the case where the response is not valid Base64 or cannot be decoded\n",
    "            print(\"Response content could not be decoded from Base64\")\n",
    "            print(response.content)\n",
    "    else:\n",
    "        print(\"Response content is empty\")\n",
    "    data = response.content\n",
    "    print(data)\n",
    "except requests.exceptions.HTTPError as http_err:\n",
    "    print(f\"HTTP error occurred: {http_err}\")\n",
    "except requests.exceptions.RequestException as err:\n",
    "    print(f\"Error occurred: {err}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cancel or track a fine-tune job or delete a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List 10 fine-tuning jobs\n",
    "client.fine_tuning.jobs.list(limit=10)\n",
    "\n",
    "# Retrieve the state of a fine-tune\n",
    "client.fine_tuning.jobs.retrieve(\"ftjob-abc123\")\n",
    "\n",
    "# Cancel a job\n",
    "client.fine_tuning.jobs.cancel(\"ftjob-abc123\")\n",
    "\n",
    "# List up to 10 events from a fine-tuning job\n",
    "client.fine_tuning.jobs.list_events(fine_tuning_job_id=\"ftjob-abc123\", limit=10)\n",
    "\n",
    "# Delete a fine-tuned model (must be an owner of the org the model was created in)\n",
    "client.models.delete(\"ft:gpt-3.5-turbo:acemeco:suffix:abc123\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using fine-tuned model via Playground or via code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a request to the new model\n",
    "completion = client.chat.completions.create(\n",
    "  model=\"ft:gpt-3.5-turbo:my-org:custom_suffix:id\",\n",
    "  messages=[\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Hello!\"}\n",
    "  ]\n",
    ")\n",
    "print(completion.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics of the model\n",
    "\n",
    "- while an active fine-tuning job is running via an event object\n",
    "- or when job is finished via querying the job and retrieving result files as csv\n",
    "- or by defining own eval method, see resources from OpenAI here: https://github.com/openai/evals\n",
    "- OR use own classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieving event object (ongoing job)\n",
    "{\n",
    "    \"object\": \"fine_tuning.job.event\",\n",
    "    \"id\": \"ftevent-abc-123\",\n",
    "    \"created_at\": 1693582679,\n",
    "    \"level\": \"info\",\n",
    "    \"message\": \"Step 300/300: training loss=0.15, validation loss=0.27, full validation loss=0.40\",\n",
    "    \"data\": {\n",
    "        \"step\": 300,\n",
    "        \"train_loss\": 0.14991648495197296,\n",
    "        \"valid_loss\": 0.26569826706596045,\n",
    "        \"total_steps\": 300,\n",
    "        \"full_valid_loss\": 0.4032616495084362,\n",
    "        \"train_mean_token_accuracy\": 0.9444444179534912,\n",
    "        \"valid_mean_token_accuracy\": 0.9565217391304348,\n",
    "        \"full_valid_mean_token_accuracy\": 0.9089635854341737\n",
    "    },\n",
    "    \"type\": \"metrics\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vote id in training und test data und in prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping malformed item: Left Maj875\n",
      "Skipping malformed item: Co\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Cäcilia\\AppData\\Local\\Temp\\ipykernel_3048\\2671900495.py:30: FutureWarning: errors='ignore' is deprecated and will raise in a future version. Use to_numeric without passing `errors` and catch exceptions explicitly instead\n",
      "  df = df.apply(pd.to_numeric, errors='ignore')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ECR%</th>\n",
       "      <th>EPP%</th>\n",
       "      <th>EFD/IDG%</th>\n",
       "      <th>Greens/EFA%</th>\n",
       "      <th>NI%</th>\n",
       "      <th>REG%</th>\n",
       "      <th>S&amp;D%</th>\n",
       "      <th>The Left%</th>\n",
       "      <th>General Majority</th>\n",
       "      <th>Left Majority</th>\n",
       "      <th>Right Majority</th>\n",
       "      <th>Consensus</th>\n",
       "      <th>Left MajS&amp;D%</th>\n",
       "      <th>Coority</th>\n",
       "      <th>EPnsensus</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.839</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.562</td>\n",
       "      <td>0.932</td>\n",
       "      <td>0.294</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.890</td>\n",
       "      <td>0.731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.563</td>\n",
       "      <td>0.784</td>\n",
       "      <td>0.269</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.759</td>\n",
       "      <td>0.803</td>\n",
       "      <td>0.688</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.590</td>\n",
       "      <td>0.802</td>\n",
       "      <td>0.188</td>\n",
       "      <td>0.875</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.636</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.568</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.846</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.815</td>\n",
       "      <td>0.620</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.636</td>\n",
       "      <td>0.787</td>\n",
       "      <td>0.295</td>\n",
       "      <td>0.852</td>\n",
       "      <td>0.273</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.813</td>\n",
       "      <td>0.568</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.846</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.568</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.813</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.846</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.423</td>\n",
       "      <td>0.892</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.731</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.828</td>\n",
       "      <td>0.898</td>\n",
       "      <td>0.303</td>\n",
       "      <td>0.080</td>\n",
       "      <td>0.156</td>\n",
       "      <td>0.830</td>\n",
       "      <td>0.772</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.824</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.333</td>\n",
       "      <td>0.910</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.896</td>\n",
       "      <td>0.853</td>\n",
       "      <td>0.080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.714</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.867</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.714</td>\n",
       "      <td>0.728</td>\n",
       "      <td>0.375</td>\n",
       "      <td>0.885</td>\n",
       "      <td>0.219</td>\n",
       "      <td>0.846</td>\n",
       "      <td>0.786</td>\n",
       "      <td>0.609</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.867</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.438</td>\n",
       "      <td>0.810</td>\n",
       "      <td>0.154</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.854</td>\n",
       "      <td>0.731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.898</td>\n",
       "      <td>0.935</td>\n",
       "      <td>0.625</td>\n",
       "      <td>0.957</td>\n",
       "      <td>0.542</td>\n",
       "      <td>0.865</td>\n",
       "      <td>0.901</td>\n",
       "      <td>0.766</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ECR%   EPP%  EFD/IDG%  Greens/EFA%    NI%   REG%   S&D%  The Left%  \\\n",
       "0   0.839  0.898     0.562        0.932  0.294  0.892  0.890      0.731   \n",
       "1   0.563  0.784     0.269        0.815  0.167  0.759  0.803      0.688   \n",
       "2   0.590  0.802     0.188        0.875  0.154  0.844  0.815      0.620   \n",
       "3   0.636  0.787     0.295        0.852  0.273  0.795  0.813      0.568   \n",
       "4   0.846  0.898     0.423        0.892  0.308  0.844  0.885      0.731   \n",
       "5   0.828  0.898     0.303        0.080  0.154  0.844  0.815      0.620   \n",
       "6   0.636  0.787     0.295        0.852  0.273  0.795  0.813      0.568   \n",
       "7   0.846  0.898     0.423        0.892  0.308  0.844  0.885      0.731   \n",
       "8   0.828  0.898     0.303        0.080  0.156  0.830  0.772      0.568   \n",
       "9   0.846  0.898     0.423        0.892  0.308  0.844  0.885      0.731   \n",
       "10  0.828  0.898     0.303        0.080  0.156  0.830  0.772      0.060   \n",
       "11  0.824  0.853     0.333        0.910  0.154  0.896  0.853      0.080   \n",
       "12  0.828  0.898     0.303        0.080  0.156  0.830  0.772      0.060   \n",
       "13  0.824  0.853     0.333        0.910  0.154  0.896  0.853      0.080   \n",
       "14  0.824  0.853     0.333        0.910  0.154  0.896  0.853      0.080   \n",
       "15  0.714  0.728     0.375        0.885  0.219  0.846  0.786      0.609   \n",
       "16  0.867    NaN       NaN          NaN    NaN    NaN    NaN        NaN   \n",
       "17  0.714  0.728     0.375        0.885  0.219  0.846  0.786      0.609   \n",
       "18  0.867  0.901     0.438        0.810  0.154  0.839  0.854      0.731   \n",
       "19  0.898  0.935     0.625        0.957  0.542  0.865  0.901      0.766   \n",
       "\n",
       "    General Majority  Left Majority  Right Majority  Consensus  Left MajS&D%  \\\n",
       "0                1.0            0.0             0.0        0.0           NaN   \n",
       "1                0.0            0.0             0.0        0.0           NaN   \n",
       "2                1.0            0.0             0.0        0.0           NaN   \n",
       "3                1.0            0.0             0.0        0.0           NaN   \n",
       "4                1.0            0.0             0.0        0.0           NaN   \n",
       "5                1.0            0.0             0.0        0.0           NaN   \n",
       "6                1.0            0.0             0.0        0.0           NaN   \n",
       "7                1.0            0.0             0.0        0.0           NaN   \n",
       "8                1.0            0.0             0.0        0.0         0.813   \n",
       "9                1.0            0.0             0.0        0.0           NaN   \n",
       "10               0.0            0.0             0.0        0.0           NaN   \n",
       "11               1.0            0.0             0.0        0.0           NaN   \n",
       "12               0.0            0.0             0.0        0.0           NaN   \n",
       "13               1.0            1.0             0.0        0.0           NaN   \n",
       "14               1.0            1.0             0.0        0.0           NaN   \n",
       "15               0.0            0.0             0.0        0.0           NaN   \n",
       "16               NaN            NaN             NaN        NaN           NaN   \n",
       "17               0.0            0.0             0.0        0.0           NaN   \n",
       "18               0.0            0.0             0.0        0.0           NaN   \n",
       "19               1.0            0.0             0.0        0.0           NaN   \n",
       "\n",
       "    Coority  EPnsensus  \n",
       "0       NaN        NaN  \n",
       "1       NaN        NaN  \n",
       "2       NaN        NaN  \n",
       "3       NaN        NaN  \n",
       "4       NaN        NaN  \n",
       "5       NaN        NaN  \n",
       "6       NaN        NaN  \n",
       "7       NaN        NaN  \n",
       "8       NaN        NaN  \n",
       "9       NaN        NaN  \n",
       "10      NaN        NaN  \n",
       "11      NaN        NaN  \n",
       "12      NaN        NaN  \n",
       "13      0.0        NaN  \n",
       "14      NaN        NaN  \n",
       "15      NaN        NaN  \n",
       "16      NaN        0.0  \n",
       "17      NaN        NaN  \n",
       "18      NaN        NaN  \n",
       "19      NaN        NaN  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Convert list ouput back to a dataframe\n",
    "\n",
    "response_ls = ['ECR%:0.839, EPP%:0.898, EFD/IDG%:0.562, Greens/EFA%:0.932, NI%:0.294, REG%:0.892, S&D%:0.89, The Left%:0.731, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.563, EPP%:0.784, EFD/IDG%:0.269, Greens/EFA%:0.815, NI%:0.167, REG%:0.759, S&D%:0.803, The Left%:0.688, General Majority:0, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.59, EPP%:0.802, EFD/IDG%:0.188, Greens/EFA%:0.875, NI%:0.154, REG%:0.844, S&D%:0.815, The Left%:0.62, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.636, EPP%:0.787, EFD/IDG%:0.295, Greens/EFA%:0.852, NI%:0.273, REG%:0.795, S&D%:0.813, The Left%:0.568, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.846, EPP%:0.898, EFD/IDG%:0.423, Greens/EFA%:0.892, NI%:0.308, REG%:0.844, S&D%:0.885, The Left%:0.731, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.828, EPP%:0.898, EFD/IDG%:0.303, Greens/EFA%:0.08, NI%:0.156, REG%:0.83, S&D%:0.772, The Left%:0.06, General Majority:0, Left Maj875, NI%:0.154, REG%:0.844, S&D%:0.815, The Left%:0.62, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.636, EPP%:0.787, EFD/IDG%:0.295, Greens/EFA%:0.852, NI%:0.273, REG%:0.795, S&D%:0.813, The Left%:0.568, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.846, EPP%:0.898, EFD/IDG%:0.423, Greens/EFA%:0.892, NI%:0.308, REG%:0.844, S&D%:0.885, The Left%:0.731, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.828, EPP%:0.898, EFD/IDG%:0.303, Greens/EFA%:0.08, NI%:0.156, REG%:0.83, S&D%:0.772, The Left%:0.06, General Majority:0, Left MajS&D%:0.813, The Left%:0.568, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.846, EPP%:0.898, EFD/IDG%:0.423, Greens/EFA%:0.892, NI%:0.308, REG%:0.844, S&D%:0.885, The Left%:0.731, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.828, EPP%:0.898, EFD/IDG%:0.303, Greens/EFA%:0.08, NI%:0.156, REG%:0.83, S&D%:0.772, The Left%:0.06, General Majority:0, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.824, EPP%:0.853, EFD/IDG%:0.333, Greens/EFA%:0.91, NI%:0.154, REG%:0.896, S&D%:0.853, The Left%:0.08, General Majority:1, Left Majority:1, Right Majority:0, Co, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.828, EPP%:0.898, EFD/IDG%:0.303, Greens/EFA%:0.08, NI%:0.156, REG%:0.83, S&D%:0.772, The Left%:0.06, General Majority:0, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.824, EPP%:0.853, EFD/IDG%:0.333, Greens/EFA%:0.91, NI%:0.154, REG%:0.896, S&D%:0.853, The Left%:0.08, General Majority:1, Left Majority:1, Right Majority:0, Coority:0, Right Majority:0, Consensus:0', 'ECR%:0.824, EPP%:0.853, EFD/IDG%:0.333, Greens/EFA%:0.91, NI%:0.154, REG%:0.896, S&D%:0.853, The Left%:0.08, General Majority:1, Left Majority:1, Right Majority:0, Consensus:0', 'ECR%:0.714, EPP%:0.728, EFD/IDG%:0.375, Greens/EFA%:0.885, NI%:0.219, REG%:0.846, S&D%:0.786, The Left%:0.609, General Majority:0, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.867, EPnsensus:0', 'ECR%:0.714, EPP%:0.728, EFD/IDG%:0.375, Greens/EFA%:0.885, NI%:0.219, REG%:0.846, S&D%:0.786, The Left%:0.609, General Majority:0, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.867, EPP%:0.901, EFD/IDG%:0.438, Greens/EFA%:0.81, NI%:0.154, REG%:0.839, S&D%:0.854, The Left%:0.731, General Majority:0, Left Majority:0, Right Majority:0, Consensus:0', 'ECR%:0.898, EPP%:0.935, EFD/IDG%:0.625, Greens/EFA%:0.957, NI%:0.542, REG%:0.865, S&D%:0.901, The Left%:0.766, General Majority:1, Left Majority:0, Right Majority:0, Consensus:0']\n",
    "# Function to convert each string into a dictionary\n",
    "def parse_string_ls_to_dict(s):\n",
    "    result = {}\n",
    "    # delete all ' from string\n",
    "    s = s.replace(\"'\", \"\")\n",
    "    # split key value paris from each other\n",
    "    for item in s.split(', '):\n",
    "        # split party names from values\n",
    "        parts = item.split(':')\n",
    "        if len(parts) == 2:\n",
    "            # turn key value pairs into dictionary\n",
    "            key, value = parts\n",
    "            result[key] = value\n",
    "        else:\n",
    "            print(f\"Skipping incorrectly formatted item: {item}\")\n",
    "    return result\n",
    "\n",
    "# Convert list of strings to list of dictionaries\n",
    "response_dicts = [parse_string_ls_to_dict(item) for item in response_ls]\n",
    "\n",
    "# Convert list of dictionaries to DataFrame\n",
    "response_df = pd.DataFrame(response_dicts)\n",
    "\n",
    "# Convert numeric columns to float\n",
    "response_df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "display(response_df)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = fu.get_accuracy_df(response_df, 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
